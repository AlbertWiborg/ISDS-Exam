{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68d3e833-7e1b-4328-aec7-f34a531e88ca",
   "metadata": {},
   "source": [
    "# Section 4: API Pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e18dd1d-0734-420d-9f2f-0fdb6c3875f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') #Ignores warning messages that take up a lot of space\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "def get_id_list(limit):\n",
    "    #Objective: get a list of the most popular musical artists' Spotify ID's from Spotify.\n",
    "    #We search for specific characters and then extract the first results using Spotify's search.\n",
    "    #Input (limit) is the amount of artists we want to extract for each letter.\n",
    "    \n",
    "    url = \"https://spotify23.p.rapidapi.com/search/\" #API location\n",
    "    \n",
    "    error_duplicates = 0 #For future error tracking\n",
    "\n",
    "    alphabet = list('abcdefghijklmnopqrstuvwxyz') #Generate list of all alphabet characters\n",
    "    alphabet2 = [] #Empty list\n",
    "    for x in alphabet:\n",
    "        for y in alphabet:\n",
    "            alphabet2.append(x+y) #Append combination of each character\n",
    "    \n",
    "    char = alphabet2  #search words characters\n",
    "    id_list = [] #List for storage of artist ID's\n",
    "    for j, i in enumerate(char):\n",
    "        querystring = {\"q\": i, \"type\": \"artists\", \"offset\": \"0\", \"limit\": limit, \"numberOfTopResults\": \"5\"}\n",
    "        headers = {\n",
    "            \"X-RapidAPI-Key\": \"a92fd231acmsh813dd13da05c575p1a3cbajsnd7dae53fb312\",\n",
    "            \"X-RapidAPI-Host\": \"spotify23.p.rapidapi.com\"\n",
    "        }\n",
    "        response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "        response_json = response.json() #Locating relevant values is easier in JSON\n",
    "\n",
    "        items = response_json['artists']['items'] #Isolates ID value in the given JSON file\n",
    "        for x in items:\n",
    "            id = x['data']['uri'] #Defines location of Spotify ID string\n",
    "            id_list.append(id[15:]) #Extracts the unique, relevant characters of Spotify ID string by slicing\n",
    "        if j%50 == 0:\n",
    "            print(f'Pulled {j+1} of {len(char)}')\n",
    "    localnum = len(id_list) #For error tracking\n",
    "    id_list = [*set(id_list)] #Removes duplicate strings in our ID list\n",
    "    error_duplicates = localnum - len(id_list) #Error tracking by taking difference of values in list before and after removing duplicates\n",
    "    id_len = len(char)*limit\n",
    "    \n",
    "    return id_list, error_duplicates, id_len\n",
    "\n",
    "#API Pull loop from id_list found in get_id_list\n",
    "def pull_data(string):\n",
    "    #Objective: Takes input string (ID from ID list) and returns all information that is\n",
    "    #displayed on the artist's Spotify page. Returns output in JSON format.\n",
    "    \n",
    "    url = \"https://spotify23.p.rapidapi.com/artist_overview/\" #Same API, just a different endpoint\n",
    "    querystring = {\"id\":string}\n",
    "\n",
    "    headers = {\n",
    "    \"X-RapidAPI-Key\": \"a92fd231acmsh813dd13da05c575p1a3cbajsnd7dae53fb312\",\n",
    "    \"X-RapidAPI-Host\": \"spotify23.p.rapidapi.com\"\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "    response_json = response.json()\n",
    "    \n",
    "    return response_json\n",
    "\n",
    "json_list = [] #Storage of artists' JSON from pull_data function \n",
    "limit = 1 #For use in get_id_list function. It is the amount of artists we want for each character.\n",
    "id_list, error_duplicates, id_len = get_id_list(limit) #We call the get_id_list function\n",
    "\n",
    "#Create JSON list of artists via pull_data\n",
    "for i, idx in enumerate(id_list):\n",
    "    json_list.append(pull_data(idx))\n",
    "    print(f'API pull number {i+1} of {len(id_list)}') #For monitoring status during API pulls.\n",
    "print(\"Success: Pulled all id's from list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65d77ff-1648-471c-8c5d-665317aa8fda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "artist_data_indices = ['Name', \n",
    "                       'ID', \n",
    "                       'Instagram', \n",
    "                       'Wikipedia', \n",
    "                       'SpotifyFollowers', \n",
    "                       'MonthlyListeners', \n",
    "                       'Singles', \n",
    "                       'TopTrackPlays', \n",
    "                       'EarliestAlbum', \n",
    "                       'MonthSinceRelease', \n",
    "                       'Albums',\n",
    "                       'AppearsOn', \n",
    "                       'DiscoveredOn'] #Names of columns in resulting DataFrame\n",
    "\n",
    "def get_external(json_file, website):\n",
    "    #Objective: Return link to artist's external links if they exist, and -1 if it doesn't exist\n",
    "    #Input: json_file - JSON file of artist's data. website - string of the website we want, and \n",
    "    # is either 'instagram' or 'wikipedia'.\n",
    "    \n",
    "    #Locate correct path in JSON file. This path leads to a list of dictionaries.\n",
    "    for item in json_file['data']['artist']['profile']['externalLinks']['items']: #Loop over external links\n",
    "        for key, value in item.items(): #We are now in the dictionary\n",
    "            if website in value: #Checks if either 'instagram' or 'wikipedia' is in value of dictionary\n",
    "                value = value.removesuffix('/?hl=en') #Removes unnecessary suffix from website URL\n",
    "                return value\n",
    "    return -1 #If input 'website' is not locatable in artist's external links, return -1\n",
    "\n",
    "def duplicate_external(dataframe):\n",
    "    #Objective: Count amount of duplicate external links. For error tracking.\n",
    "    duplicate_count = 0 #Amount of duplicate external links for artist.\n",
    "    for row in range(len(result)): #Want to compare a given artist's instagram to wikipedia link, so we get index.\n",
    "        #In our case, the if statement only evaluates to True if both columns for artist is equal to -1\n",
    "        if result.iloc[row]['Wikipedia'] == result.iloc[row]['Instagram']:\n",
    "            duplicate_count += 1 #Iterate error count\n",
    "    return duplicate_count\n",
    "\n",
    "def earliest_album_release(json_file):\n",
    "    #Objective: Get all album releases for given artist, convert values to format that allows us\n",
    "    #to compare dates and return the lowest value corresponding to the earliest album release\n",
    "    \n",
    "    #First we specify where to find the albums\n",
    "    path = json_file['data']['artist']['discography']['albums']['items']\n",
    "    if json_file['data']['artist']['discography']['albums']['totalCount'] == 0:\n",
    "        return -1\n",
    "    releasedate_list = []\n",
    "    #Because some artists have multiple albums, we loop over each album to find the release dates\n",
    "    for i in range(len(path)):\n",
    "        year = path[i]['releases']['items'][0]['date']['year']\n",
    "        month = path[i]['releases']['items'][0]['date']['month']\n",
    "        if month == None:\n",
    "            month = 12\n",
    "        #If month contains a single digit we add a leading zero by adding a lacking zero to the year variable\n",
    "        if month//10 == 0:\n",
    "            year = int(str(year) + '0')\n",
    "        day = path[i]['releases']['items'][0]['date']['day']\n",
    "        if day == None:\n",
    "            day = 30\n",
    "        #If day contains a single digit we add a leading zero by adding a lacking zero to the month variable\n",
    "        if day//10 == 0:\n",
    "            month = int(str(month) + '0')\n",
    "        date = int(str(year) + str(month) + str(day))\n",
    "        releasedate_list.append(date)\n",
    "    #Return lowest integer value of album release dates\n",
    "    releasedate_min = min(releasedate_list)\n",
    "    releasedate_min = str(releasedate_min)[:4]+'-'+str(releasedate_min)[4:6]+'-'+str(releasedate_min)[6:8]\n",
    "    return releasedate_min\n",
    "\n",
    "def diff_month(start):\n",
    "    #Objective: Function returning months since album release. Uses 2022-08-23 as anchor.\n",
    "    #Input: Date in the format YYYY-MM-DD.\n",
    "    if start == -1:\n",
    "        return -1 #If no album release, return -1\n",
    "    finish = '2022-08-23'\n",
    "    #Returns 12*year difference + month difference\n",
    "    return (int(finish[:4]) - int(start[:4]))*12 + int(finish[5:7]) - int(start[5:7])\n",
    "\n",
    "def toptrack(json_file):\n",
    "    #Objective: Of all top tracks released by the artist, we want to find the track with the most total plays\n",
    "    path = json_file['data']['artist']['discography']['topTracks']['items'] #Locate path\n",
    "    if len(path) == 0:\n",
    "        return -1\n",
    "    playcount = [] #List to append playcounts\n",
    "    for index in range(len(path)): #Loop over all top tracks in list\n",
    "        playcount.append(int(path[index]['track']['playcount'])) #The value is in string format, so we convert to int\n",
    "    return max(playcount) #Returns highest playcount of the top tracks\n",
    "\n",
    "def artist_data(json_file):\n",
    "    #Returns a list of data of specific artist from JSON file\n",
    "    resulting_list = []\n",
    "    resulting_list.append(json_file['data']['artist']['profile']['name']) #Profile name\n",
    "    resulting_list.append(json_file['data']['artist']['id']) #Spotify ID\n",
    "    resulting_list.append(get_external(json_file, 'instagram')) #Instagram link\n",
    "    resulting_list.append(get_external(json_file, 'wikipedia')) #Wikipedia link\n",
    "    resulting_list.append(json_file['data']['artist']['stats']['followers']) #SpotifyFollowers\n",
    "    resulting_list.append(json_file['data']['artist']['stats']['monthlyListeners']) #MonthlyListeners\n",
    "    resulting_list.append(json_file['data']['artist']['discography']['singles']['totalCount']) #Singles\n",
    "    resulting_list.append(toptrack(json_file)) #TopTrackPlays\n",
    "    resulting_list.append(earliest_album_release(json_file)) #EarliestAlbum\n",
    "    resulting_list.append(diff_month(earliest_album_release(json_file)))\n",
    "    resulting_list.append(json_file['data']['artist']['discography']['albums']['totalCount']) #Albums\n",
    "    \n",
    "    related = ['appearsOn', 'discoveredOn'] #Strings for input in JSON pathing\n",
    "    for string in related:\n",
    "        resulting_list.append(json_file['data']['artist']['relatedContent'][string]['totalCount']) #Appears- and DiscoveredOn\n",
    "    return resulting_list\n",
    "\n",
    "result = pd.DataFrame(columns = artist_data_indices) #DataFrame for all artists' data with column names\n",
    "\n",
    "error_warnings = 0 #For error tracking.\n",
    "\n",
    "for i, json_file in enumerate(json_list):\n",
    "    #Some API pulls result in errors, so we keep track of these and skip them in our iterations\n",
    "    if ('error' in json_list[i]) or ('errors' in json_list[i]):\n",
    "        error_warnings += 1\n",
    "        continue\n",
    "    if i%1000 == 0:\n",
    "        print(f'Appended {i*1000} artists to result')\n",
    "    result.loc[i] = artist_data(json_file)\n",
    "\n",
    "tempvar = len(result) #For error tracking\n",
    "#Drop rows of artists that are very small\n",
    "result = result.drop(result[(result.TopTrackPlays < 20000)].index)\n",
    "error_nonArtists = tempvar - len(result) #Error tracking of artists that are very small.\n",
    "error_duplicatelinks = duplicate_external(result) #Amount of overlapping missing external links\n",
    "tempvar = len(result)\n",
    "result = result.drop(result[(result.Instagram == -1) | (result.Wikipedia == -1)].index) #Drops missing external links\n",
    "error_missingExternal = tempvar - len(result)\n",
    "result = result.reset_index() #Reset index\n",
    "del result['index'] #Delete excess index column\n",
    "instagram_list = list(result.Instagram) #Extract instagram links\n",
    "print(f'Number of artists pulled: {id_len}')\n",
    "print(f'Number of duplicate artists: {error_duplicates}')\n",
    "print(f'Number of API pulls resulting in error: {error_warnings}')\n",
    "print(f'Number of artists with less than 1000 monthly listeners dropped: {error_nonArtists}')\n",
    "print(f'Number of missing external links: {error_missingExternal} ({error_duplicatelinks} overlapping Wikipedia and Instagram missing links)')\n",
    "print(f'Resulting amount of artists: {id_len - error_warnings - error_nonArtists - (id_len-len(json_list)) - error_missingExternal}')\n",
    "result.to_csv('API.result.csv', index = False)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae888a72-219b-4d61-b374-224c363745c8",
   "metadata": {},
   "source": [
    "# Section 4: Wikipedia Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbf27b6-c6fb-402b-8f14-fafddfe9635b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "wiki_list = pd.read_csv(r'API.result.csv')\n",
    "wiki_list = wiki_list['Wikipedia']\n",
    "\n",
    "# Scraping the infobox of wiki - Maybe this should be defined as a function\n",
    "\n",
    "def wikiscrape(wiki_list):\n",
    "    #Objective: Scrap each wikipage in wiki_list for genres and origin, calculate the amount of years, that\n",
    "    #the artist has been active and return dataframe with these values.\n",
    "    df = pd.DataFrame(columns=[0,1,2,3]) #Empty dataframe\n",
    "\n",
    "    data = [] #outside loop, empty list for each artist that we concatenate on the resulting list\n",
    "\n",
    "    for i,x in enumerate(wiki_list):\n",
    "        try:\n",
    "            infoboxes = pd.read_html(x, index_col=0, attrs={\"class\":\"infobox\"})\n",
    "        except: #If no infobox present in wikipage, skip this iteration\n",
    "            continue\n",
    "\n",
    "        data = [x] #Set first element of list to wikipedia link\n",
    "        #In the infobox, info about origin is either labeled as 'Origin' or 'Born'\n",
    "        try: #Tries Origin, this is the expression when looking up a band\n",
    "            test = infoboxes[0].xs('Origin').values[0]\n",
    "            test = test.split(\"[\")[0] #Remove suffix\n",
    "            test = re.sub(r'^.+, ([^/]+)$', r'\\1', test) #Takes everything efter ', ' to get only country\n",
    "            data.append(test) #Appends origin values\n",
    "        except: #Tries born, this is the expression when looking up a single artist\n",
    "            try: \n",
    "                test = infoboxes[0].xs('Born').values[0]\n",
    "                test = re.sub(r'^.+\\)([^/]+)$', r'\\1', test) #Takes everything after ')'\n",
    "                test = re.sub(r'^.+\\d([^/]+)$', r'\\1', test) #Takes everything after the last number\n",
    "                test = re.sub(r'^.+]([^/]+)$', r'\\1', test) #Takes everything after ']'\n",
    "                test = test.split(\"[\")[0] #Remove suffix\n",
    "                test = re.sub(r'^.+, ([^/]+)$', r'\\1', test) #Takes everything after ', ' to get only country\n",
    "                data.append(test) #Appends born values\n",
    "            except:\n",
    "                data.append(-1) \n",
    "        try: #This part of the code secures that we only take the first genre mentioned on Wiki for each artist\n",
    "            #test = infoboxes[0].xs('Genres').values[0]\n",
    "            #data.append(test)\n",
    "            response = requests.get(x, headers={'name':'Albert Wiborg','email':'ptd207@alumni.ku.dk'})\n",
    "            soup = BeautifulSoup(response.content, 'lxml')\n",
    "            for row in soup.table.tbody: #Loops over the rows in the table on the wikipage\n",
    "                if 'Genres' in row.text: #If a row contains 'Genres' in the text\n",
    "                    genres = row.find_all(\"li\") #Sets list equal to all genres in the row\n",
    "                    if len(genres) == 0: #If no genres was found (could be due to genres being links)\n",
    "                        genres = row.find_all(\"a\") #We set genres equal to the links in the row\n",
    "                    genres = genres[0].text #We only want the first genre\n",
    "                    genres = genres.split(',')[0] #Additional argument to remove subsequent genres\n",
    "                    genres = genres.strip('[]') #Remove source-suffixes\n",
    "            data.append(genres)\n",
    "        except: \n",
    "            data.append(-1)\n",
    "        try:\n",
    "            test = str(infoboxes[0].xs('Years active').values[0])\n",
    "            data.append(test)\n",
    "        except:\n",
    "            data.append(-1)\n",
    "        data = pd.DataFrame(data)\n",
    "        data = data.transpose() #Converting it from a long to a wide dataset\n",
    "        df = pd.concat([df,data])\n",
    "        print(f'try: {i+1} out of {len(wiki_list)}')\n",
    "    return df\n",
    "df = wikiscrape(wiki_list)\n",
    "df.to_csv('Wiki_data_Raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb064d8-a4b4-4933-b6d5-855b92a8d761",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(r'Wiki_data_Raw.csv')\n",
    "df = df.iloc[:,1:]\n",
    "df = df.rename({'0': 'Wikipedia', '1': 'Origin', '2' : 'Genre' , '3': 'Years_Active' }, axis='columns')\n",
    "\n",
    "# Calculating the active years as the difference between 2022 and the first year of the career\n",
    "for i in range(len(df)):\n",
    "    df.iloc[i][3] = re.sub(\"^\\\\D+(\\\\d)\", \"\\\\1\", str(df.iloc[i][3])) # Removing everything until the first digit of the string\n",
    "\n",
    "df['Years_Active'] = pd.to_numeric(df['Years_Active'].str[:4])\n",
    "df = df.drop(df[(df['Years_Active'] == '1') | (df['Years_Active'] == 1)].index)    \n",
    "df['Years_Active'] = np.where((df['Years_Active'] != 'NaN'), 2022 - df['Years_Active'], -1)\n",
    "\n",
    "# Cleaning the genre column from references placed on wiki\n",
    "df['Genre'] = df['Genre'].str.split('[').str[0]\n",
    "#Making everything lowercase\n",
    "df['Genre'] = df['Genre'].str.lower()\n",
    "\n",
    "df['Genre'] = df['Genre'].map(str) # Converting to a string to use the find opperation\n",
    "\n",
    "def check_genre(genre):   \n",
    "## The purpose of this function is to classify genres from wiki into a more general framework\n",
    "## Genre catagories: Hip hop, Pop, R&B, Rock, Country, EDM, Funk, Dance music\n",
    "## If the orriginal genre contains any of these words in the main genre list, the genre will be replaced with the word from main genre list\n",
    "    main_genres = ['hip hop', 'pop', 'r&b', 'rock', 'country', 'electronic music', 'dance music', 'folk']\n",
    "    for word in main_genres:\n",
    "        if genre.find(word) != -1:\n",
    "            return word\n",
    "\n",
    "def check_genre_manually(genre):\n",
    "\n",
    "## The purpose of this function is to classify genres from wiki into a more general framework\n",
    "## Genre catagories: Hip hop, Pop, R&B, Rock, Country, EDM, Funk, Dance music\n",
    "    \n",
    "    if ('rap' in genre) | (genre == 'trap') | (genre == 'sampledelia') | (genre == 'hiplife') | (genre == 'freestyle') | \\\n",
    "         ('hip-hop' in genre) | (genre == 'nerdcore') | (genre == 'igbo highlife') | (genre == 'trip hop') | (genre == 'grime') |\\\n",
    "         (genre == 'urbano music'): # Defining rap as Hip Hop\n",
    "        return 'hip hop'\n",
    "\n",
    "\n",
    "    elif (genre == 'reggaeton') | (genre == 'reggaetón') | (genre == 'reggae') | (genre == 'reggae fusion') | (genre == 'roots reggae') |\\\n",
    "         (genre == 'bachata') | ('eurodance' in genre) | ('dance' in genre) | (genre == 'disco') | ('salsa' in genre) | (genre == 'funk') |\\\n",
    "         (genre == 'urbano') | ('afrobeat' in genre) | (genre == 'electro') | (genre == 'ballad') | (genre == 'son') | (genre == 'idm') |\\\n",
    "         (genre == 'nu-disco') | (genre == 'kizomba') | (genre == 'duranguense') | (genre =='son cubano'):\n",
    "         return 'dance music'\n",
    "    \n",
    "    \n",
    "    elif ('metal' in genre) | ('punk' in genre) | (genre == \"black 'n' roll\") | (genre == 'screamo') | \\\n",
    "         (genre == 'melodic hardcore') | (genre == 'indie folk') | ( 'grunge' in genre) | (genre == 'post-hardcore') | \\\n",
    "         (genre == 'emo') | (genre == 'alternative') | (genre == 'deathcore') | (genre == 'crossover thrash') | \\\n",
    "         (genre == 'dark ambient') | (genre == 'grupero') | (genre == 'slowcore') | (genre == 'powerviolence') | (genre == 'vada vada') |\\\n",
    "         (genre == 'indie'): # Defining metal as Rock\n",
    "        return 'rock'\n",
    "    \n",
    "\n",
    "    elif ('soul' in genre) | (genre == 'tejano') | ('jazz' in genre) | ('blues' in genre) | (genre == 'stride') | (genre == 'ska') |\\\n",
    "         (genre == 'latin ballad') | (genre == 'hard bop') | (genre == '2 tone'):\n",
    "        return 'r&b'\n",
    "    \n",
    "    elif (genre == 'techno') | (genre == 'edm') | ('house' in genre) | (genre == 'future bass') | \\\n",
    "         (genre == 'progressive trance') | (genre == 'dubstep') | ('trance' in genre) | (genre == 'hardstyle') | \\\n",
    "         (genre == 'ebm') | (genre == 'glitch') | (genre == 'drum and bass') | (genre == 'big beat') | \\\n",
    "         ('ambient' in genre) | (genre == 'aggrotech') | (genre == 'experimental') | (genre == 'future funk') | \\\n",
    "         (genre == 'tribal-guarachero') | (genre == 'kwaito') | (genre == 'banku') | (genre == 'indietronica') | \\\n",
    "         (genre == 'indietronica') | (genre == 'new-age') | (genre == 'synthwave') | (genre == 'video game music') |\\\n",
    "         (genre == 'glitch hop') | ('electronic' in genre) | (genre == 'amapiano') | (genre == 'gqom') | (genre == 'instrumental'):\n",
    "        return 'electronic music'\n",
    "        \n",
    "\n",
    "    \n",
    "    elif (genre == 'americana') |  (genre == 'gulf and western') | (genre == 'red dirt') | (genre == 'old-time'):\n",
    "        return 'country'\n",
    "    \n",
    "    \n",
    "    elif (genre == 'ethiopian music') | (genre == 'vocal') | ('minimal' in genre) | (genre == 'new wave') | \\\n",
    "         (genre == 'novelty') | (genre == 'chillwave') | (genre == 'acoustic') | (genre == 'ethereal wave') | \\\n",
    "         (genre == 'avant-garde') | (genre == 'playback singing') | (genre == \"children's\") | (genre == 'disney music') | \\\n",
    "         (genre == 'latin') | (genre == \"children's music\") :\n",
    "        return 'pop'\n",
    "\n",
    "    \n",
    "    elif (genre == 'musical theatre') | (genre == 'broadway') | ('film' in genre) | (genre == 'bollywood') | \\\n",
    "         ('contemporary' in genre) | ('worship' in genre) | (genre == 'ccm') | ('gospel' in genre) | \\\n",
    "         (genre == 'vallenato') | (genre == 'punjabi') | (genre == 'modern laika') | (genre == 'qawwali') | \\\n",
    "         (genre == 'qawwali') | (genre == 'norteño') | (genre == 'sea shanties') | (genre == 'world') | \\\n",
    "         (genre == 'opm') | (genre == 'a cappella') | (genre == 'opera') | (genre == 'classical') | \\\n",
    "         (genre == 'classical crossover') | (genre == 'indian classical music') | ('irish' in genre) | (genre == 'soca') |\\\n",
    "         (genre == 'new flamenco') | (genre == 'fado') | (genre == 'roots') | (genre == 'canadiana') | (genre == 'bhangra') |\\\n",
    "         (genre == 'joik') | (genre == 'sufi'):\n",
    "        return 'folk'\n",
    "\n",
    "    \n",
    "    else:\n",
    "        return check_genre(genre)\n",
    "\n",
    "df['Main.Genre'] = df['Genre'].apply(check_genre_manually)\n",
    "df['Main.Genre'] = df['Main.Genre'].fillna(value = df['Genre'])\n",
    "\n",
    "# Dropping nan values\n",
    "df = df.drop(df[(df['Main.Genre'] == 'nan') | (df['Main.Genre'] == '') | (df['Main.Genre']=='-1')].index)\n",
    "# Oveview of the genre distribution after manipulation\n",
    "y = df['Main.Genre']\n",
    "y2 = y.value_counts()\n",
    "for ind,val in y2.iteritems():\n",
    "    print(ind,val)\n",
    "\n",
    "# Cleaning Origin column\n",
    "\n",
    "# Replacing american states with \"United States\"\n",
    "state_names=[\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\"]\n",
    "\n",
    "for i in df['Origin']:\n",
    "    for x in state_names:\n",
    "        if x in i:\n",
    "            df['Origin'] = np.where((df['Origin'] == i), 'United States', df['Origin'])\n",
    "\n",
    "# Replace U.S. with united states\n",
    "df['Origin'] = np.where(((df['Origin']== 'Brigham Young University')  | (df['Origin'] == 'US') | (df['Origin']== 'U.S.') |\\\n",
    "(df['Origin'] == \"U.S. JIDEducationStephenson High SchoolHampton University (no degree\") | ( df['Origin'] == 'U.S') |\\\n",
    "(df['Origin'] == 'USA') | (df['Origin'] == 'CA') | (df['Origin'] == 'Los Angeles') | (df['Origin'] == 'S.D.') |\\\n",
    "(df['Origin'] == 'OK') | (df['Origin'] == 'IL') | (df['Origin'] == 'Howard University')) \\\n",
    ", 'United States', df['Origin'])\n",
    "\n",
    "# If any of the contries is in the Origin column replace with country\n",
    "country_names = ['Afghanistan', 'Albania', 'Algeria', 'Andorra', 'Angola', 'Antigua & Deps', 'Argentina', 'Armenia', 'Australia', 'Austria',\\\n",
    "'Azerbaijan', 'Bahamas', 'Bahrain', 'Bangladesh', 'Barbados', 'Belarus', 'Belgium', 'Belize', 'Benin', 'Bhutan', 'Bolivia',\\\n",
    "'Bosnia Herzegovina', 'Botswana', 'Brazil', 'Brunei', 'Bulgaria', 'Burkina', 'Burundi', 'Cambodia', 'Cameroon', 'Canada', \\\n",
    "'Cape Verde', 'Central African Rep', 'Chad', 'Chile', 'China', 'Colombia', 'Comoros', 'Congo', 'Congo', 'Costa Rica', 'Croatia',\\\n",
    "'Cuba', 'Cyprus', 'Czech Republic', 'Denmark', 'Djibouti', 'Dominica', 'Dominican Republic', 'East Timor', 'Ecuador', 'England', 'Egypt',\\\n",
    "'El Salvador', 'Equatorial Guinea', 'Eritrea', 'Estonia', 'Ethiopia', 'Fiji', 'Finland', 'France', 'Gabon', 'Gambia', 'Georgia', \\\n",
    "'Germany', 'Ghana', 'Greece', 'Grenada', 'Guatemala', 'Guinea', 'Guinea-Bissau', 'Guyana', 'Haiti', 'Honduras', 'Hungary', 'Iceland',\\\n",
    "'India', 'Indonesia', 'Iran', 'Iraq', 'Ireland', 'Israel', 'Italy', 'Ivory Coast', 'Jamaica', 'Japan', 'Jordan', 'Kazakhstan',\\\n",
    "'Kenya', 'Kiribati', 'Korea North', 'Korea South', 'Kosovo', 'Kuwait', 'Kyrgyzstan', 'Laos', 'Latvia', 'Lebanon', 'Lesotho',\\\n",
    "'Liberia', 'Libya', 'Liechtenstein', 'Lithuania', 'Luxembourg', 'Macedonia', 'Madagascar', 'Malawi', 'Malaysia', 'Maldives',\\\n",
    "'Mali', 'Malta', 'Marshall Islands', 'Mauritania', 'Mauritius', 'Mexico', 'Micronesia', 'Moldova', 'Monaco', 'Mongolia', \\\n",
    "'Montenegro', 'Morocco', 'Mozambique', 'Myanmar', 'Namibia', 'Nauru', 'Nepal', 'Netherlands', 'New Zealand', 'Nicaragua',\\\n",
    "'Niger', 'Nigeria', 'Norway', 'Oman', 'Pakistan', 'Palau', 'Panama', 'Papua New Guinea', 'Paraguay', 'Peru', 'Philippines', \\\n",
    "'Poland', 'Portugal', 'Qatar', 'Romania', 'Russia', 'Rwanda', 'St Kitts & Nevis', 'St Lucia', 'Saint Vincent & the Grenadines',\\\n",
    "'Samoa', 'San Marino', 'Sao Tome & Principe', 'Saudi Arabia', 'Senegal', 'Serbia', 'Seychelles', 'Sierra Leone', 'Singapore',\\\n",
    "'Slovakia', 'Slovenia', 'Solomon Islands', 'Somalia', 'South Africa', 'South Sudan', 'Spain', 'Sri Lanka', 'Sudan', 'Suriname', \\\n",
    "'Swaziland', 'Sweden', 'Switzerland', 'Syria', 'Taiwan', 'Tajikistan', 'Tanzania', 'Thailand', 'Togo', 'Tonga', 'Trinidad & Tobago',\\\n",
    "'Tunisia', 'Turkey', 'Turkmenistan', 'Tuvalu', 'Uganda', 'Ukraine', 'United Arab Emirates', 'United Kingdom', 'United States',\\\n",
    "'Uruguay', 'Uzbekistan', 'Vanuatu', 'Vatican City', 'Venezuela', 'Vietnam', 'Yemen', 'Zambia', 'Zimbabwe']\n",
    "\n",
    "for i in df['Origin']:\n",
    "    for x in country_names:\n",
    "        if x in i:\n",
    "            df['Origin'] = np.where((df['Origin'] == i), x, df['Origin'])\n",
    "\n",
    "            \n",
    "# Replace Punjab with india\n",
    "df['Origin'] = np.where((df['Origin'] == 'Punjab') | (df['Origin'] == 'Kerala'), 'India', df['Origin'])\n",
    "# Replace England, London with United Kingdom\n",
    "df['Origin'] = np.where((df['Origin'] == 'England') | (df['Origin'] == 'London') | (df['Origin'] == 'Pembury') | (df['Origin'] == 'UK') | (df['Origin'] == 'Wales') | (df['Origin'] == 'Wiltshire'), 'United Kingdom', df['Origin'])\n",
    "# Replace Soviet union with rusia\n",
    "df['Origin'] = np.where( (df['Origin'] == 'Soviet Union'), 'Russia', df['Origin'])\n",
    "df['Origin'] = np.where( (df['Origin'] == 'Rusia'), 'Russia', df['Origin'])\n",
    "# Replacing for Australia\n",
    "df['Origin'] = np.where( (df['Origin'] == \"St Kevin's CollegeMonash University\"), 'Australia', df['Origin'])\n",
    "# Replacing all with Yugoslavia which will be dropped afterwards\n",
    "df['Origin'] = np.where( (df['Origin'] == \"FPR Yugoslavia\") | (df['Origin'] == \"SFR Yugoslavia\"), 'Yugoslavia', df['Origin'])\n",
    "df['Origin'] = np.where(df['Origin'] == 'Yugoslavia', '-1', df['Origin'])\n",
    "# Replacing Toronto with canada\n",
    "df['Origin'] = np.where((df['Origin'] == 'Toronto') | (df['Origin'] == 'AB') | (df['Origin'] == 'British Columbia'), 'Canada', df['Origin'])\n",
    "\n",
    "# Dropping relevant rows such as empty and rows labeled with -1\n",
    "df = df.drop(df[df['Origin'] == '-1'].index)\n",
    "df = df.drop(df[df['Origin'] == 'Ziwerekoru Fumudoh'].index)\n",
    "df = df.drop(df[df['Origin'] == ''].index)\n",
    "#df.drop(['Unnamed: 0.1','Unnamed: 0'], axis=1, inplace=True)\n",
    "df['Origin'] = df['Origin'].str.strip('()[]')\n",
    "df = df.drop(df[(df['Origin'] == 'nan') | (df['Origin'] == '')].index)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c1932c-6991-402f-b935-7038f42aaf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dummies for english and spanish speaking countries\n",
    "df['dum_eng'] = np.where((df['Origin'] == 'United States') | (df['Origin'] == 'United Kingdom') | (df['Origin'] == 'Australia') | (df['Origin'] == 'Canada') | (df['Origin'] == 'New Zealand'), 1, 0)\n",
    "df['dum_spa'] = np.where((df['Origin'] == 'Spain') | (df['Origin'] == 'Argentina') | (df['Origin'] == 'Bolivia') | (df['Origin'] == 'Chile') | (df['Origin'] == 'Colombia') | (df['Origin'] == 'Costa Rica') | (df['Origin'] == 'Cuba') \\\n",
    "| (df['Origin'] == 'Dominican Republic') | (df['Origin'] == 'Ecuador') | (df['Origin'] == 'El Salvador') | (df['Origin'] == 'Equatorial Guinea')| (df['Origin'] == 'Guatemala') \\\n",
    "| (df['Origin'] == 'Honduras') | (df['Origin'] == 'Mexico') | (df['Origin'] == 'Nicaragua') | (df['Origin'] == 'Panama')\\\n",
    "| (df['Origin'] == 'Paraguay') | (df['Origin'] == 'Peru') | (df['Origin'] == 'Puerto Rico') | (df['Origin'] == 'Uruguay')\\\n",
    "| (df['Origin'] == 'Venezuela'), 1, 0)\n",
    "#Create dummies for country and genres for use in machine learning\n",
    "df['dum_country'] = np.where(df['Main.Genre'] == 'country', 1, 0)\n",
    "df['dum_dance'] = np.where(df['Main.Genre'] == 'dance music', 1, 0)\n",
    "df['dum_electronic'] = np.where(df['Main.Genre'] == 'electronic music', 1, 0)\n",
    "df['dum_folk'] = np.where(df['Main.Genre'] == 'folk', 1, 0)\n",
    "df['dum_hiphop'] = np.where(df['Main.Genre'] == 'hip hop', 1, 0)\n",
    "df['dum_r&b'] = np.where(df['Main.Genre'] == 'r&b', 1, 0)\n",
    "df['dum_r&b'] = np.where(df['Main.Genre'] == 'rock', 1, 0)\n",
    "df.to_csv(r'Wikipedia.result.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87fb203-4563-4974-b2f4-1a9cfb4199ba",
   "metadata": {},
   "source": [
    "# Section 4: Instagram Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce53d6df-9f88-401d-81b2-0700db76b39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "url =  'https://www.instagram.com'\n",
    "\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.get(url)\n",
    "\n",
    "time.sleep(3)\n",
    "# Accepting cookies\n",
    "cookie = driver.find_element(By.CSS_SELECTOR, '.bIiDR')\n",
    "cookie.click()\n",
    "\n",
    "time.sleep(3)\n",
    "# Logging into instagram\n",
    "username = 'oscarkarlsson0804'\n",
    "code = 'isds123!'\n",
    "\n",
    "username_input = driver.find_element(By.CSS_SELECTOR, \"input[name='username']\")\n",
    "password_input = driver.find_element(By.CSS_SELECTOR, \"input[name='password']\")\n",
    "\n",
    "username_input.send_keys(username)\n",
    "password_input.send_keys(code)\n",
    "\n",
    "login_button = driver.find_element(By.XPATH, \"//button[@type='submit']\")\n",
    "login_button.click()\n",
    "\n",
    "time.sleep(5)\n",
    "safe_login = driver.find_element(By.CSS_SELECTOR, '.y3zKF')\n",
    "safe_login.click()\n",
    "\n",
    "time.sleep(5)\n",
    "noti_no = driver.find_element(By.CSS_SELECTOR, '._a9_1')\n",
    "noti_no.click()\n",
    "\n",
    "df = pd.read_csv('API.result.csv')\n",
    "insta_list = df['Instagram']\n",
    "insta_list = pd.DataFrame(insta_list)\n",
    "insta_list = insta_list.drop(insta_list[insta_list.Instagram == '-1'].index)\n",
    "\n",
    "follower_list=[]\n",
    "col = ['Instagram', 'I_Followers']\n",
    "follower_df=pd.DataFrame()\n",
    "import re\n",
    "for i in insta_list['Instagram']:\n",
    "    try:\n",
    "        driver.get(i)\n",
    "        time.sleep(5)\n",
    "        soup2 = BeautifulSoup(driver.page_source, 'lxml')\n",
    "        followers = soup2.find_all('div', class_ = '_aacl _aacp _aacu _aacx _aad6 _aade')\n",
    "        followers = str(followers[1])\n",
    "    except:\n",
    "        follower_list.append(-1)\n",
    "        continue\n",
    "\n",
    "    followers = followers.replace(',','')\n",
    "    All_digits = re.findall(r'\\d+', followers)\n",
    "    All_digits = All_digits[2]\n",
    "    follower_list.append(All_digits)\n",
    "\n",
    "insta_list_df = insta_list.reset_index(drop=True)\n",
    "insta_list_df['I_Followers']=follower_list\n",
    "\n",
    "insta_list_df.to_csv('Instagram.result.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9679193-1c1a-45a5-a1c0-c811ce1d8931",
   "metadata": {},
   "source": [
    "# Section 4: File merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d04dee3-a246-4b88-81d1-237f68354f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This section merges the different DataFrames that we have created in the 3 other sections\n",
    "final=pd.read_csv('API.result.csv')\n",
    "Instagram_df=pd.read_csv('Instagram.result.csv')\n",
    "Wiki_df=pd.read_csv('Wikipedia.result.csv')\n",
    "final=final.merge(Instagram_df, how='left', on='Instagram' )\n",
    "final=final.merge(Wiki_df, how='left', on='Wikipedia' )\n",
    "\n",
    "#Drop irrelevant columns\n",
    "final = final.drop(columns='Unnamed: 0_y')\n",
    "final = final.dropna()\n",
    "final = final.drop(final[final.I_Followers == -1].index)\n",
    "final = final.drop(final[final.Origin == -1].index)\n",
    "final = final.drop(final[final.Genre == -1].index)\n",
    "final = final.drop(final[final.Years_Active == -1].index)\n",
    "final = final.drop(final[final.EarliestAlbum == -1].index)\n",
    "final = final.drop_duplicates(subset = 'ID', keep = 'first', ignore_index = True)\n",
    "final = final.reset_index()\n",
    "\n",
    "final.to_csv('final.csv')\n",
    "final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7790f245-ee53-4333-aead-24732bf87534",
   "metadata": {},
   "source": [
    "# Country-map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5a379965-af92-4c73-abed-66bb0a625ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wiki = pd.read_csv('Wikipedia.result.csv')\n",
    "del df_wiki['Unnamed: 0']\n",
    "\n",
    "countries = []\n",
    "counts = []\n",
    "#Since value_counts returns a series, we use iteritems() so we can access these with our variables val and ind\n",
    "for val, ind in df_wiki['Origin'].value_counts().iteritems():\n",
    "    countries.append(val)\n",
    "    counts.append(ind)\n",
    "\n",
    "def country_genre(df2):\n",
    "    #Objective: Get the most common genre from each country\n",
    "    #Input: Dataframe (Wiki_data converted to DataFrame in particular)\n",
    "    df = df2.copy() #Copy DataFrame so that we don't overwrite the input DataFrame outside of this function\n",
    "    genres = [] #List to contain genres\n",
    "    unique_countries = df_wiki[\"Origin\"].nunique() #Set equal to amount of unique values in Origin column\n",
    "    \n",
    "    for i in range(unique_countries):\n",
    "        #Groupby groups countries by Main Genre. We then count the values of each genre by country\n",
    "        #and set s equal to the maximum of all countries and genres, fx Rock USA\n",
    "        s = df.groupby('Main.Genre')['Origin'].value_counts().idxmax() \n",
    "        genre = s[0] #s is a tuple, with the genre in index = 0\n",
    "        country = s[1] #and country in index = 1\n",
    "        genres.append(genre) #append genre from tuple to list of genres which we want to be returned\n",
    "        df.drop(df.loc[df['Origin'] == country].index, inplace = True) #we now drop the country whose top genre we just extracted and repeat the process\n",
    "    \n",
    "    return genres\n",
    "\n",
    "#Make new DataFrame with data of amount of artists from each country and corresponding most popular genres in the countries \n",
    "country_map = pd.DataFrame({\"Country\": countries, \"Count\": counts, \"Most popular Genre\": country_genre(df_wiki)})\n",
    "country_map.to_csv(\"country_map.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6a46c3-4061-4776-b468-c0a9eb77a325",
   "metadata": {},
   "source": [
    "# Country map plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5995b7b0-a9af-4125-9b41-8253de985961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px #download plotly package\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import kaleido #pip install kaleido\n",
    "import os\n",
    "import geopandas as gpd #pip install geopandas\n",
    "\n",
    "final=pd.read_csv('final.csv')\n",
    "# importing our data set of countries/origin, most frequent genre and number of observation for each Origin\n",
    "df=pd.read_csv('country_map.csv')\n",
    "df['Country'] = np.where(df['Country'] == 'United States', 'United States of America', df['Country'])\n",
    "df['Country'] = np.where(df['Country'] == 'Czech Republic', 'Czechia', df['Country'])\n",
    "df['Most popular Genre'] = np.where(df['Most popular Genre'] == 'electronic music', 'EDM', df['Most popular Genre'] )\n",
    "df['Most popular Genre'] = np.where(df['Most popular Genre'] == 'dance music', 'Dance', df['Most popular Genre'] )\n",
    "df['Most popular Genre'] = np.where(df['Most popular Genre'] == 'pop', 'Pop', df['Most popular Genre'] )\n",
    "df['Most popular Genre'] = np.where(df['Most popular Genre'] == 'r&b', 'R&B', df['Most popular Genre'] )\n",
    "df['Most popular Genre'] = np.where(df['Most popular Genre'] == 'rock', 'Rock', df['Most popular Genre'] )\n",
    "df['Most popular Genre'] = np.where(df['Most popular Genre'] == 'folk', 'Folk', df['Most popular Genre'] )\n",
    "df['Most popular Genre'] = np.where(df['Most popular Genre'] == 'country', 'Country', df['Most popular Genre'] )\n",
    "df['Most popular Genre'] = np.where(df['Most popular Genre'] == 'hip hop', 'Rap', df['Most popular Genre'] )\n",
    "\n",
    "# Importing data set with geo location data\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "\n",
    "final=world.merge(df, how='inner', right_on='Country', left_on = 'name' )\n",
    "final['Count2'] = np.log(final['Count'])\n",
    "final = final.sort_values(by = 'Count',  ascending=False)\n",
    "fig, ax = plt.subplots(figsize=(17, 8))\n",
    "final.plot(column = \"Count2\",ax=ax, legend=False, cmap='Greens')\n",
    "plt.axis('off')\n",
    "final.apply(lambda x: ax.annotate(text=x['Most popular Genre'], xy=x.geometry.centroid.coords[0], horizontalalignment='center', fontsize = 6), axis=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7300c677-f534-4757-bcaa-76a81db85298",
   "metadata": {},
   "source": [
    "# Plots of genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237bdc2d-9c3a-4ce4-8d65-baaa9c867426",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import plotly.express as px #download plotly package using: $ pip install plotly==5.10.0\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df=pd.read_csv('final.csv', index_col=0)\n",
    "\n",
    "removals = df['Origin'].value_counts().reset_index()\n",
    "removals = removals[removals['Origin'] > 5]['index'].values\n",
    "origin_df = df[df['Origin'].isin(removals)]\n",
    "df['Origin'].value_counts()\n",
    "fig9 = px.histogram(origin_df, x=\"Main.Genre\", color=\"Origin\", labels={'Main.Genre':'Main genres'} # color of histogram bars\n",
    "                   )\n",
    "fig9.update_layout(width=1000, height=700, bargap=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dade55-5b56-4e51-a726-bb97ff9884c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RandB = df[df['Main.Genre']=='r&b']\n",
    "DanceMusic = df[df['Main.Genre']=='dance music']\n",
    "Pop = df[df['Main.Genre']=='pop']\n",
    "Rock = df[df['Main.Genre']=='rock']\n",
    "Folk = df[df['Main.Genre']=='folk']\n",
    "Country = df[df['Main.Genre']=='country']\n",
    "Electronic = df[df['Main.Genre']=='electronic']\n",
    "HipHip = df[df['Main.Genre']=='hip hop']\n",
    "\n",
    "df = df[df.Albums < 80]\n",
    "df = df[df.MonthlyListeners < 60000000]\n",
    "df = df[df.Years_Active != 2021]\n",
    "\n",
    "fig19 = px.histogram(df, x=\"Main.Genre\", \n",
    "                   histnorm='percent',\n",
    "                  color_discrete_sequence=['seagreen'],\n",
    "                    nbins=40, text_auto=True)\n",
    "\n",
    "fig19.update_layout(xaxis_title=\"Main Genres\", yaxis_title=\"Pct. of the data set\")\n",
    "fig19.update_layout(width=1300, height=800, bargap=0.05)\n",
    "fig19.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d64cefe-9226-4de0-839d-00ef9a50b013",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig19 = px.histogram(df, x=\"MonthlyListeners\", \n",
    "                   histnorm='percent',\n",
    "                  color_discrete_sequence=['seagreen'],\n",
    "                    nbins=50, text_auto=True)\n",
    "\n",
    "fig19.update_layout(xaxis_title=\"MonthlyListeners\", yaxis_title=\"Pct. of the data set\")\n",
    "fig19.update_layout(width=1300, height=800, bargap=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034c1e3d-d818-4c0c-9089-14207271a354",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Figure 3 plots\n",
    "\n",
    "fig1 = px.scatter(df, x='TopTrackPlays', y='MonthlyListeners', trendline=\"ols\", color_discrete_sequence=[\"green\"])\n",
    "fig1.show()\n",
    "\n",
    "fig2 = px.scatter(df, x='AppearsOn', y='MonthlyListeners', trendline=\"ols\", color_discrete_sequence=[\"green\"])\n",
    "fig2.show()\n",
    "\n",
    "fig3 = px.scatter(df, x='MonthSinceRelease', y='MonthlyListeners', trendline=\"ols\", color_discrete_sequence=[\"green\"])\n",
    "fig3.show()\n",
    "\n",
    "fig4 = px.scatter(df, x='Albums', y='MonthlyListeners', trendline=\"ols\", color_discrete_sequence=[\"green\"])\n",
    "fig4.show()\n",
    "\n",
    "fig5 = px.scatter(df, x='I_Followers', y='MonthlyListeners', trendline=\"ols\", color_discrete_sequence=[\"green\"])\n",
    "fig5.show()\n",
    "\n",
    "fig6 = px.scatter(df, x='Years_Active', y='MonthlyListeners', trendline=\"ols\", color_discrete_sequence=[\"green\"])\n",
    "fig6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08c64d0-f78c-4c07-94cc-d1977ad505fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig16 = px.scatter(df, x='TopTrackPlays', y='MonthlyListeners', trendline=\"ols\", facet_col='Main.Genre', facet_col_wrap=9, facet_row_spacing=0.062500, color_discrete_sequence=[\"green\"])\n",
    "fig16.update_layout(width=1300, height=500, bargap=0.05)\n",
    "fig16.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888e91df-a25d-4d1d-9dcf-74259bb54c25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Figure 4 plots\n",
    "\n",
    "UnitedStates  = origin_df[origin_df['Origin']=='United States']\n",
    "UnitedKingdom  = origin_df[origin_df['Origin']=='United Kingdom']\n",
    "Australia = origin_df[origin_df['Origin']=='Australia']\n",
    "\n",
    "fig6 = px.scatter_polar(UnitedStates,theta='Main.Genre', r='MonthlyListeners', color = 'Origin')\n",
    "fig6.update_layout(width=800, height=800, bargap=0.05)\n",
    "fig6.show()\n",
    "\n",
    "fig7 = px.scatter_polar(UnitedKingdom,theta='Main.Genre', r='MonthlyListeners', color = 'Origin')\n",
    "fig7.update_layout(width=800, height=800, bargap=0.05)\n",
    "fig7.show()\n",
    "\n",
    "fig8 = px.scatter_polar(Australia,theta='Main.Genre', r='MonthlyListeners', color = 'Origin')\n",
    "fig8.update_layout(width=800, height=800, bargap=0.05)\n",
    "fig8.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bc5414-a2c6-4202-9b72-96961c6f2e6e",
   "metadata": {},
   "source": [
    "# Section 6: Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac07915b-18d9-4a8a-8f1b-a7bf77b82ef9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing relevant packages\n",
    "import math\n",
    "import matplotlib #install matplotlimb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# LOAD FROM SCIKIT-LEARN\n",
    "from sklearn.linear_model import LogisticRegression #install scikit-learn\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from sklearn.metrics import mean_squared_error as mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5bf418-530f-4a23-a67d-1a25364af48e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "DATA SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d85e5e98-d2b5-4979-b1cc-5d93f7f3e132",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "final = pd.read_csv('final.csv')\n",
    "final = final.drop(final[(final['I_Followers'] == 0) & (final['I_Followers'] == -1)].index)\n",
    "final = final.drop(final[(final['MonthlyListeners'] > 20000000)].index)\n",
    "final['log_I_Followers']=np.log(final['I_Followers'])\n",
    "final=final.sample(frac=1) #shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0d00cc50-ef2b-4e17-826a-e436decfcbe4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# DEFINE FEATURES AND TARGET\n",
    "X = np.array(final[['SpotifyFollowers',\n",
    "                    'TopTrackPlays',\n",
    "                    'AppearsOn',\n",
    "                    'log_I_Followers',\n",
    "                    'dum_eng',\n",
    "                    'dum_spa',\n",
    "                    'dum_country',\n",
    "                    'dum_dance',\n",
    "                    'dum_electronic',\n",
    "                    'dum_folk',\n",
    "                    'dum_hiphop',\n",
    "                    'dum_r&b'\n",
    "                   ]]) # features\n",
    "\n",
    "y = np.array(final['MonthlyListeners']) # target\n",
    "\n",
    "# SPLIT INTO DEVELOPMENT (2/3) AND TEST DATA (1/3)\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X, y, test_size=1/3, random_state=14)\n",
    "\n",
    "# SPLIT DEVELOPMENT INTO TRAIN (1/3) AND VALIDATION (1/3)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_dev, y_dev, test_size=1/2, random_state=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e16284-b647-4379-9dab-14a6b96cbf61",
   "metadata": {},
   "source": [
    "#### Model 1: LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "11d32685-f9ff-4d66-8a20-7d4672de2103",
   "metadata": {},
   "outputs": [],
   "source": [
    "perform = [] # Store performance\n",
    "lambdas = np.logspace(-4, 4, 50) # Grid of lambdas\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kfolds = KFold(n_splits=10)\n",
    "folds = list(kfolds.split(X_dev, y_dev))\n",
    "\n",
    "# Outer loop: lambdas\n",
    "mseCV = []\n",
    "for lambda_ in lambdas:\n",
    "\n",
    "    # Inner loop: folds\n",
    "    mseCV_ = []\n",
    "    for train_idx, val_idx in folds:\n",
    "\n",
    "        # Train model and compute MSE on test fold\n",
    "        pipe_lassoCV = make_pipeline(PolynomialFeatures(degree=2, include_bias=True),\n",
    "                                     StandardScaler(),\n",
    "                                     Lasso(alpha=lambda_, random_state=14))\n",
    "        X_train, y_train = X_dev[train_idx], y_dev[train_idx]\n",
    "        X_val, y_val = X_dev[val_idx], y_dev[val_idx]\n",
    "        pipe_lassoCV.fit(X_train, y_train)\n",
    "        mseCV_.append(mse(pipe_lassoCV.predict(X_val), y_val))\n",
    "\n",
    "    # Store result\n",
    "    mseCV.append(mseCV_)\n",
    "\n",
    "# Convert to DataFrame\n",
    "lambdaCV = pd.DataFrame(mseCV, index=lambdas)\n",
    "\n",
    "\n",
    "# CHOOSE OPTIMAL HYPERPARAMETERS (mean of MSE's across folds)\n",
    "optimal_lambda = lambdaCV.mean(axis=1).nsmallest(1)\n",
    "\n",
    "# RETRAIN/RE-ESTIMATE MODEL USING OPTIMAL HYPERPARAMETERS AND COMPARE PERFORMANCE\n",
    "pipe_lassoCV = make_pipeline(PolynomialFeatures(include_bias=False),\n",
    "                             StandardScaler(),\n",
    "                             Lasso(alpha=optimal_lambda.index[0], random_state=14))\n",
    "\n",
    "pipe_lassoCV.fit(X_dev,y_dev) #fit optimal lambda to entire development set: likely to improve performance slightly since we use more oberservations\n",
    "\n",
    "lambda_lasso=optimal_lambda.index[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6f979c48-f659-40c3-9bfd-b752d0160e1e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Outer loop: lambdas\n",
    "mseCV = []\n",
    "for lambda_ in lambdas:\n",
    "\n",
    "    # Inner loop: folds\n",
    "    mseCV_ = []\n",
    "    for train_idx, val_idx in folds:\n",
    "\n",
    "        # Train model and compute MSE on test fold\n",
    "        pipe_lassoCV_3 = make_pipeline(PolynomialFeatures(degree=3, include_bias=True),\n",
    "                                     StandardScaler(),\n",
    "                                     Lasso(alpha=lambda_, random_state=14))\n",
    "        X_train, y_train = X_dev[train_idx], y_dev[train_idx]\n",
    "        X_val, y_val = X_dev[val_idx], y_dev[val_idx]\n",
    "        pipe_lassoCV_3.fit(X_train, y_train)\n",
    "        mseCV_.append(mse(pipe_lassoCV_3.predict(X_val), y_val))\n",
    "\n",
    "    # Store result\n",
    "    mseCV.append(mseCV_)\n",
    "\n",
    "# Convert to DataFrame\n",
    "lambdaCV = pd.DataFrame(mseCV, index=lambdas)\n",
    "\n",
    "\n",
    "# CHOOSE OPTIMAL HYPERPARAMETERS (mean of MSE's across folds)\n",
    "optimal_lambda = lambdaCV.mean(axis=1).nsmallest(1)\n",
    "\n",
    "# RETRAIN/RE-ESTIMATE MODEL USING OPTIMAL HYPERPARAMETERS AND COMPARE PERFORMANCE\n",
    "pipe_lassoCV_3 = make_pipeline(PolynomialFeatures(include_bias=False),\n",
    "                             StandardScaler(),\n",
    "                             Lasso(alpha=optimal_lambda.index[0], random_state=14))\n",
    "\n",
    "pipe_lassoCV_3.fit(X_dev,y_dev) #fit optimal lambda to entire development set: likely to improve performance slightly since we use more oberservations\n",
    "\n",
    "lambda_lasso_3=optimal_lambda.index[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9aa294d-9c3d-4459-bcd9-0f8ed85ef149",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Model 2: RIDGE w. cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "033cc0c4-143b-48d7-b08e-24875aa1b4e9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "# Outer loop: lambdas\n",
    "mseCV = []\n",
    "for lambda_ in lambdas:\n",
    "\n",
    "    # Inner loop: folds\n",
    "    mseCV_ = []\n",
    "    for train_idx, val_idx in folds:\n",
    "\n",
    "        # Train model and compute MSE on test fold\n",
    "        pipe_ridgeCV = make_pipeline(PolynomialFeatures(degree=2, include_bias=True),\n",
    "                                     StandardScaler(),\n",
    "                                     Ridge(alpha=lambda_, random_state=14))\n",
    "        X_train, y_train = X_dev[train_idx], y_dev[train_idx]\n",
    "        X_val, y_val = X_dev[val_idx], y_dev[val_idx]\n",
    "        pipe_ridgeCV.fit(X_train, y_train)\n",
    "        mseCV_.append(mse(pipe_ridgeCV.predict(X_val), y_val))\n",
    "\n",
    "    # Store result\n",
    "    mseCV.append(mseCV_)\n",
    "\n",
    "# Convert to DataFrame\n",
    "lambdaCV = pd.DataFrame(mseCV, index=lambdas)\n",
    "\n",
    "\n",
    "# CHOOSE OPTIMAL HYPERPARAMETERS (mean of MSE's across folds)\n",
    "optimal_lambda = lambdaCV.mean(axis=1).nsmallest(1)\n",
    "\n",
    "# RETRAIN/RE-ESTIMATE MODEL USING OPTIMAL HYPERPARAMETERS AND COMPARE PERFORMANCE\n",
    "pipe_ridgeCV = make_pipeline(PolynomialFeatures(include_bias=False),\n",
    "                             StandardScaler(),\n",
    "                             Ridge(alpha=optimal_lambda.index[0], random_state=14))\n",
    "\n",
    "pipe_ridgeCV.fit(X_dev,y_dev) #fit optimal lambda to entire development set: likely to improve performance slightly since we use more oberservations\n",
    "\n",
    "lambda_ridge=optimal_lambda.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7098bfbc-3ab0-4380-a0a4-e26e2341ac05",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mseCV = []\n",
    "for lambda_ in lambdas:\n",
    "\n",
    "    # Inner loop: folds\n",
    "    mseCV_ = []\n",
    "    for train_idx, val_idx in folds:\n",
    "\n",
    "        # Train model and compute MSE on test fold\n",
    "        pipe_ridgeCV_3 = make_pipeline(PolynomialFeatures(degree=3, include_bias=True),\n",
    "                                     StandardScaler(),\n",
    "                                     Ridge(alpha=lambda_, random_state=14))\n",
    "        X_train, y_train = X_dev[train_idx], y_dev[train_idx]\n",
    "        X_val, y_val = X_dev[val_idx], y_dev[val_idx]\n",
    "        pipe_ridgeCV_3.fit(X_train, y_train)\n",
    "        mseCV_.append(mse(pipe_ridgeCV_3.predict(X_val), y_val))\n",
    "\n",
    "    # Store result\n",
    "    mseCV.append(mseCV_)\n",
    "\n",
    "# Convert to DataFrame\n",
    "lambdaCV_3 = pd.DataFrame(mseCV, index=lambdas)\n",
    "\n",
    "\n",
    "# CHOOSE OPTIMAL HYPERPARAMETERS (mean of MSE's across folds)\n",
    "optimal_lambda = lambdaCV_3.mean(axis=1).nsmallest(1)\n",
    "\n",
    "# RETRAIN/RE-ESTIMATE MODEL USING OPTIMAL HYPERPARAMETERS AND COMPARE PERFORMANCE\n",
    "pipe_ridgeCV = make_pipeline(PolynomialFeatures(include_bias=False),\n",
    "                             StandardScaler(),\n",
    "                             Ridge(alpha=optimal_lambda.index[0], random_state=14))\n",
    "\n",
    "pipe_ridgeCV.fit(X_dev,y_dev) #fit optimal lambda to entire development set: likely to improve performance slightly since we use more oberservations\n",
    "\n",
    "lambda_ridge_3=optimal_lambda.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c94e8f-7a5f-4570-92ed-f735deb62319",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#SCORE\n",
    "models = { 'Lasso CV': pipe_lassoCV ,'Lasso CV 3': pipe_lassoCV_3 ,'Ridge CV': pipe_ridgeCV ,'Ridge CV 3': pipe_ridgeCV_3 }\n",
    "#print('Optimal lambda:', optimal.index[0])\n",
    "\n",
    "for name, model in models.items():\n",
    "    score = math.sqrt(mse(model.predict(X_test),y_test))\n",
    "    print(name, round(score, 2))\n",
    "print()\n",
    "print('Lasso CV Lambda:', lambda_lasso )\n",
    "print('Lasso CV Lambda 3:', lambda_lasso_3 )\n",
    "print('Ridge CV Lambda:', lambda_ridge )\n",
    "print('Ridge CV Lambda 3:', lambda_ridge_3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edbd841-3896-4f87-9b64-0d1867e00a66",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Model 3: Elastic net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "035cf2e5-1620-489d-b9ae-f8786e60cc3d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "pipe_el = make_pipeline(PolynomialFeatures(degree=2,include_bias=False),\n",
    "                        StandardScaler(),\n",
    "                        ElasticNet(tol=0.001,random_state=14))\n",
    "\n",
    "gs = GridSearchCV(estimator=pipe_el,\n",
    "                  param_grid={'elasticnet__alpha':lambdas,\n",
    "                              'elasticnet__l1_ratio':np.linspace(0,1,20)},\n",
    "                  scoring='neg_mean_squared_error',\n",
    "                  cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3830e24f-0663-4a4b-a2ee-f0ddcfd20ef4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "pipe_el_3 = make_pipeline(PolynomialFeatures(degree=3,include_bias=False),\n",
    "                        StandardScaler(),\n",
    "                        ElasticNet(tol=0.001,random_state=14))\n",
    "\n",
    "gs_3 = GridSearchCV(estimator=pipe_el,\n",
    "                  param_grid={'elasticnet__alpha':lambdas,\n",
    "                              'elasticnet__l1_ratio':np.linspace(0,1,20)},\n",
    "                  scoring='neg_mean_squared_error',\n",
    "                  cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e578234-6dce-4b60-9086-19da1dffc72f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#SCORE\n",
    "models = { 'Lasso CV': pipe_lassoCV ,'Lasso CV 3': pipe_lassoCV_3 ,'Ridge CV': pipe_ridgeCV ,'Ridge CV 3': pipe_ridgeCV_3 ,'Elastic Net':gs.fit(X_dev, y_dev),'Elastic Net 3':gs_3.fit(X_dev, y_dev) }\n",
    "\n",
    "\n",
    "for name, model in models.items():\n",
    "    score = math.sqrt(mse(model.predict(X_test),y_test))\n",
    "    print(name, round(score, 2))\n",
    "print()\n",
    "print('Lasso CV Lambda:', lambda_lasso )\n",
    "print('Ridge CV Lambda:', lambda_ridge)\n",
    "print('CV Elastic Net params:', gs.best_params_)\n",
    "print('Lasso CV 3 Lambda:', lambda_lasso_3 )\n",
    "print('Ridge CV 3 Lambda:', lambda_ridge_3)\n",
    "print('CV Elastic Net 3 params:', gs_3.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626e8343-93c3-43f9-a804-d93cf7af38e5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Plots of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "92e9d72d-a41b-4f66-b36a-207bf345598c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred=pipe_ridgeCV.predict(X_test)\n",
    "#predictions\n",
    "import plotly.express as px #download plotly package\n",
    "import kaleido\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "\n",
    "p = {'y_test': y_test, 'y_pred': y_pred}\n",
    "predictions=pd.DataFrame(data=p)\n",
    "predictions\n",
    "\n",
    "fig1 = px.scatter(predictions, x='y_test', y='y_pred')\n",
    "fig2 = px.line(predictions, x=\"y_test\", y=\"y_test\") #y=x line added\n",
    "fig1.add_trace(fig2.data[0])\n",
    "\n",
    "fig1.write_image(\"Residual plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144a230c-b5d4-4f51-a1a3-055d8b39355d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# LEANINGCURVE\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, test_scores = \\\n",
    "    learning_curve(estimator=pipe_ridgeCV,\n",
    "                   X=X_dev,\n",
    "                   y=y_dev,\n",
    "                   train_sizes=np.arange(0.05, 1.05, .05),\n",
    "                   scoring='neg_mean_squared_error',\n",
    "                   cv=10)\n",
    "\n",
    "mse_ = pd.DataFrame({'Train':-train_scores.mean(axis=1),\n",
    "                     'Test':-test_scores.mean(axis=1)})\\\n",
    "        .set_index(pd.Index(train_sizes,name='sample size'))\n",
    "\n",
    "mse_.head(5)\n",
    "\n",
    "#PLot 1\n",
    "f_learn, ax = plt.subplots(figsize=(12,6))\n",
    "ax.plot(train_sizes,np.sqrt(-test_scores.mean(1)), alpha=0.25, linewidth=2, label ='Validation', color='blue') # negated, because we already use neg_MSE\n",
    "ax.plot(train_sizes,np.sqrt(-train_scores.mean(1)), alpha=0.25, linewidth=2, label='Train', color='orange') # negated, because we already use neg_MSE\n",
    "\n",
    "ax.set_title('Mean performance')\n",
    "ax.set_ylabel('Root-Mean squared error')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ae1816-8e4a-43cd-a2b8-8976881eb1c9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Plot 2\n",
    "f_learn, ax = plt.subplots(figsize=(12,7))\n",
    "ax.fill_between(train_sizes, np.sqrt(-test_scores.min(1)), np.sqrt(-test_scores.max(1)), alpha=0.25, label ='Validation', color='blue')\n",
    "\n",
    "ax.fill_between(train_sizes, np.sqrt(-train_scores.min(1)), np.sqrt(-train_scores.max(1)),  alpha=0.25, label='Train', color='orange')\n",
    "\n",
    "ax.set_title('Range of performance (min, max)')\n",
    "ax.set_ylabel('Root-Mean squared error')\n",
    "ax.legend();\n",
    "plt.savefig('Learning.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47a5471-c0d5-40cf-9cda-6862f5501233",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Change param_name and estimator!\n",
    "\n",
    "#VALIDATION CURVE\n",
    "# LOAD FROM SCIKIT-LEARN\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "# FIT AND EVALUATE FOR DIFFERENT LAMBDAS\n",
    "train_scores, test_scores = \\\n",
    "    validation_curve(estimator=pipe_ridgeCV,\n",
    "                     X=X_dev,\n",
    "                     y=y_dev,\n",
    "                     param_name='ridge__alpha',\n",
    "                     param_range=lambdas, #values to consider\n",
    "                     scoring='neg_mean_squared_error',\n",
    "                     cv=10)\n",
    "\n",
    "# OBTAIN MSE FOR DIFFERENT LAMBDAS AND PRINT BEST\n",
    "mse_score = pd.DataFrame({'Train':-train_scores.mean(axis=1),\n",
    "                          'Validation':-test_scores.mean(axis=1),\n",
    "                          'lambda':lambdas})\\\n",
    "              .set_index('lambda')\n",
    "print(mse_score.Validation.nsmallest(1))\n",
    "\n",
    "np.sqrt(mse_score).plot(logx=True, figsize=(12,7));\n",
    "\n",
    "plt.savefig('Validation.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
